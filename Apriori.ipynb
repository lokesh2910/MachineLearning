{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Association Rule - Simplified Version\n",
        "\n",
        "###What is an Association Rule?\n",
        "Imagine you work in a grocery store. You notice that whenever someone buys bread, they often buy butter too. An association rule is a way for computers to find these patterns in lots of shopping data. The rule is written like this:\n",
        "\n",
        "***If a customer buys bread, then they are likely to buy butter.***\n",
        "\n",
        "In computer terms, we write it as:\n",
        "Bread → Butter\n",
        "\n",
        "**Why is this useful?**\n",
        "\n",
        "Stores use these rules to put bread and butter close together, or to suggest butter when you buy bread online. This helps them sell more and makes shopping easier for you"
      ],
      "metadata": {
        "id": "0risHSSmyDbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apriori Algorithm - Simplified Version\n",
        "\n",
        "###What is the Apriori Algorithm?\n",
        "\n",
        "The Apriori Algorithm is a clever way for computers to find these association rules quickly, even in huge lists of shopping data. It works step by step:\n",
        "\n",
        "* Step 1: Look for items that are bought often (like bread, milk, or butter).\n",
        "\n",
        "* Step 2: Combine these to see which pairs (or triples) are often bought together.\n",
        "\n",
        "* Step 3: If a group of items (like bread and butter) is bought often, any smaller group inside it (like just bread) must also be bought often. This helps the computer skip checking groups that aren’t popular.\n",
        "\n",
        "\n",
        "**Real-life Example:**\n",
        "\n",
        "Let’s say in a week, many people buy popcorn, milk, and cereal together. The Apriori algorithm finds that not only is \"popcorn, milk, cereal\" a popular combo, but also \"popcorn, milk\" and \"milk, cereal\" are popular pairs. So, if a customer buys popcorn and milk, the store can recommend cereal.\n",
        "\n",
        "## Why is it called \"Apriori\"?\n",
        "Because it uses the idea that if a big group is popular, all the smaller groups inside it must be popular too. This saves a lot of time for the computer"
      ],
      "metadata": {
        "id": "959lvLkCyuFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Market Basket Analysis\n",
        "\n",
        "Market Basket Analysis is like being a detective for shopping carts. It helps stores figure out what products people like to buy together. The goal is to find patterns, like:\n",
        "\n",
        "* \"People who buy chips also buy soda.\"\n",
        "\n",
        "* \"If someone buys shampoo, they often buy conditioner too.\""
      ],
      "metadata": {
        "id": "8rn71PtMzYIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample Coding Trial"
      ],
      "metadata": {
        "id": "G6ZgKekU0gh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m45mU0Z1vXrf",
        "outputId": "efe1b2b7-429e-4481-b8e8-06ff4246ccfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  antecedents consequents  support  confidence    lift\n",
            "0      (beer)   (diapers)      0.6        1.00  1.2500\n",
            "1   (diapers)      (beer)      0.6        0.75  1.2500\n",
            "2   (diapers)     (bread)      0.6        0.75  0.9375\n",
            "3     (bread)   (diapers)      0.6        0.75  0.9375\n",
            "4      (milk)     (bread)      0.6        0.75  0.9375\n",
            "5     (bread)      (milk)      0.6        0.75  0.9375\n",
            "6   (diapers)      (milk)      0.6        0.75  0.9375\n",
            "7      (milk)   (diapers)      0.6        0.75  0.9375\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Example dataset: Each list is a shopping basket (transaction)\n",
        "dataset = [\n",
        "    ['milk', 'bread', 'butter'],\n",
        "    ['bread', 'diapers', 'beer', 'eggs'],\n",
        "    ['milk', 'diapers', 'beer', 'cola'],\n",
        "    ['bread', 'milk', 'diapers', 'beer'],\n",
        "    ['bread', 'milk', 'diapers', 'cola']\n",
        "]\n",
        "\n",
        "# Step 1: Convert dataset into a one-hot encoded DataFrame\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(dataset).transform(dataset)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Step 2: Apply Apriori to find frequent itemsets\n",
        "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
        "\n",
        "# Step 3: Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Step 4: Display the rules\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term           | Meaning                                                                                   | How to Interpret the Value                                                                                                               |\n",
        "|----------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Antecedents**| The \"if\" part of the rule (the item(s) you start with).                                  | If a rule says \"bread → butter\", then \"bread\" is the antecedent.                                                                        |\n",
        "| **Consequents**| The \"then\" part of the rule (the item(s) that may follow).                               | In \"bread → butter\", \"butter\" is the consequent.                                                                                        |\n",
        "| **Support**    | How often the combination (antecedent + consequent) appears in all transactions.          | High support (e.g., 0.10 or 10%) means this combo is common. Low support means it’s rare.                                               |\n",
        "| **Confidence** | How often the consequent appears when the antecedent is present (conditional probability).| High confidence (e.g., 0.80 or 80%) means that when you see the antecedent, the consequent is very likely to also be present.           |\n",
        "| **Lift**       | How much more likely the consequent is to appear with the antecedent than by chance.      | Lift > 1: strong positive association; Lift ≈ 1: no association; Lift < 1: negative association.                                        |\n"
      ],
      "metadata": {
        "id": "w87jJNT81qvG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DpA9qqG1EIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With out Built-In Functions"
      ],
      "metadata": {
        "id": "yrOlWWuA2Epg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Synthetic Market Basket Data"
      ],
      "metadata": {
        "id": "Sr9i0B522KYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "items = ['bread', 'butter', 'milk', 'eggs', 'cheese', 'tea', 'coffee', 'sugar', 'flour', 'rice', 'chocolate', 'cookies', 'juice', 'soda', 'diapers']\n",
        "num_records = 2000\n",
        "min_items = 2\n",
        "max_items = 5\n",
        "\n",
        "basket_data = []\n",
        "for _ in range(num_records):\n",
        "    basket_len = random.randint(min_items, max_items)\n",
        "    basket = random.sample(items, basket_len)\n",
        "    basket_data.append(basket)\n"
      ],
      "metadata": {
        "id": "kv6bMEMf2HfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apriori Algorithm Implementation"
      ],
      "metadata": {
        "id": "uVaQo7Ez2Qw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "min_support = 0.05  # 5%\n",
        "min_confidence = 0.6\n",
        "num_transactions = len(basket_data)\n",
        "min_support_count = min_support * num_transactions\n",
        "\n",
        "# Helper: Count support for candidate itemsets\n",
        "def get_support_count(candidates, transactions):\n",
        "    support_count = defaultdict(int)\n",
        "    for transaction in transactions:\n",
        "        transaction_set = set(transaction)\n",
        "        for candidate in candidates:\n",
        "            if candidate.issubset(transaction_set):\n",
        "                support_count[candidate] += 1\n",
        "    return support_count\n",
        "\n",
        "# Step 1: Find frequent 1-itemsets\n",
        "candidate_1_itemsets = [frozenset([item]) for item in items]\n",
        "support_count_1 = get_support_count(candidate_1_itemsets, basket_data)\n",
        "frequent_1_itemsets = set()\n",
        "for itemset, count in support_count_1.items():\n",
        "    if count >= min_support_count:\n",
        "        frequent_1_itemsets.add(itemset)\n",
        "\n",
        "# Step 2: Generate candidates for larger itemsets\n",
        "def generate_candidates(frequent_itemsets_k_minus_1):\n",
        "    candidates = set()\n",
        "    f_list = list(frequent_itemsets_k_minus_1)\n",
        "    k = len(next(iter(f_list))) + 1\n",
        "    for i in range(len(f_list)):\n",
        "        for j in range(i+1, len(f_list)):\n",
        "            union_set = f_list[i] | f_list[j]\n",
        "            if len(union_set) == k:\n",
        "                candidates.add(union_set)\n",
        "    return candidates\n",
        "\n",
        "# Step 3: Iteratively find all frequent itemsets\n",
        "current_frequent_itemsets = frequent_1_itemsets\n",
        "all_frequent_itemsets = {}\n",
        "k = 1\n",
        "while current_frequent_itemsets:\n",
        "    for itemset in current_frequent_itemsets:\n",
        "        if itemset not in all_frequent_itemsets:\n",
        "            all_frequent_itemsets[itemset] = support_count_1[itemset] if k == 1 else 0\n",
        "    k += 1\n",
        "    candidates_k = generate_candidates(current_frequent_itemsets)\n",
        "    if not candidates_k:\n",
        "        break\n",
        "    support_count_k = get_support_count(candidates_k, basket_data)\n",
        "    current_frequent_itemsets = set()\n",
        "    for itemset, count in support_count_k.items():\n",
        "        if count >= min_support_count:\n",
        "            current_frequent_itemsets.add(itemset)\n",
        "            all_frequent_itemsets[itemset] = count"
      ],
      "metadata": {
        "id": "b_QpdE8f2PGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Association Rules"
      ],
      "metadata": {
        "id": "NSOppX3J2nIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rules = []\n",
        "for itemset in all_frequent_itemsets:\n",
        "    if len(itemset) > 1:\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                if antecedent in all_frequent_itemsets:\n",
        "                    support_itemset = all_frequent_itemsets[itemset] / num_transactions\n",
        "                    support_antecedent = all_frequent_itemsets[antecedent] / num_transactions\n",
        "                    confidence = support_itemset / support_antecedent\n",
        "                    if confidence >= min_confidence:\n",
        "                        support_consequent = all_frequent_itemsets.get(consequent, 0) / num_transactions if consequent in all_frequent_itemsets else 0\n",
        "                        lift = confidence / support_consequent if support_consequent > 0 else 0\n",
        "                        rules.append({\n",
        "                            'antecedents': antecedent,\n",
        "                            'consequents': consequent,\n",
        "                            'support': support_itemset,\n",
        "                            'confidence': confidence,\n",
        "                            'lift': lift\n",
        "                        })"
      ],
      "metadata": {
        "id": "qfy25EZy2mWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Prominent Rules (Clear Groups)"
      ],
      "metadata": {
        "id": "cnIjqC6r22Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort and filter rules\n",
        "prominent_rules = [rule for rule in rules if rule['support'] >= min_support and rule['confidence'] >= 0.75]\n",
        "prominent_rules = sorted(prominent_rules, key=lambda x: (x['confidence'], x['support']), reverse=True)\n",
        "\n",
        "# print(f\"Number of prominent rules: {len(prominent_rules)}\")\n",
        "for rule in prominent_rules[:10]:  # Show top 10\n",
        "    ant = '-'.join(sorted(rule['antecedents']))\n",
        "    cons = '-'.join(sorted(rule['consequents']))\n",
        "    print(f\"Rule: {ant} -> {cons}, Support: {rule['support']:.2f}, Confidence: {rule['confidence']:.2f}, Lift: {rule['lift']:.2f}\")"
      ],
      "metadata": {
        "id": "VCH2Ne5F2t7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "def generate_market_baskets(num_records=2000, min_items=2, max_items=5):\n",
        "    items = ['bread', 'butter', 'milk', 'eggs', 'cheese', 'tea', 'coffee', 'sugar', 'flour', 'rice', 'chocolate', 'cookies', 'juice', 'soda', 'diapers']\n",
        "    basket_data = []\n",
        "    for _ in range(num_records):\n",
        "        basket_len = random.randint(min_items, max_items)\n",
        "        basket = set(random.sample(items, basket_len))\n",
        "        # Add some intentional associations\n",
        "        if random.random() < 0.4:\n",
        "            basket.update(['bread', 'butter'])  # 40% of baskets have both\n",
        "        if random.random() < 0.3:\n",
        "            basket.update(['tea', 'sugar'])     # 30% of baskets have both\n",
        "        basket_data.append(list(basket))\n",
        "    return basket_data, items\n",
        "\n",
        "def apriori_runner(min_support=0.05, min_confidence=0.6):\n",
        "    basket_data, items = generate_market_baskets()\n",
        "    num_transactions = len(basket_data)\n",
        "    min_support_count = min_support * num_transactions\n",
        "\n",
        "    def get_support_count(candidates, transactions):\n",
        "        support_count = defaultdict(int)\n",
        "        for transaction in transactions:\n",
        "            transaction_set = set(transaction)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(transaction_set):\n",
        "                    support_count[candidate] += 1\n",
        "        return support_count\n",
        "\n",
        "    candidate_1_itemsets = [frozenset([item]) for item in items]\n",
        "    support_count_1 = get_support_count(candidate_1_itemsets, basket_data)\n",
        "    frequent_1_itemsets = set()\n",
        "    for itemset, count in support_count_1.items():\n",
        "        if count >= min_support_count:\n",
        "            frequent_1_itemsets.add(itemset)\n",
        "\n",
        "    def generate_candidates(frequent_itemsets_k_minus_1):\n",
        "        candidates = set()\n",
        "        f_list = list(frequent_itemsets_k_minus_1)\n",
        "        if not f_list:\n",
        "            return candidates\n",
        "        k = len(next(iter(f_list))) + 1\n",
        "        for i in range(len(f_list)):\n",
        "            for j in range(i+1, len(f_list)):\n",
        "                union_set = f_list[i] | f_list[j]\n",
        "                if len(union_set) == k:\n",
        "                    candidates.add(union_set)\n",
        "        return candidates\n",
        "\n",
        "    current_frequent_itemsets = frequent_1_itemsets\n",
        "    all_frequent_itemsets = {}\n",
        "    k = 1\n",
        "    while current_frequent_itemsets:\n",
        "        for itemset in current_frequent_itemsets:\n",
        "            if itemset not in all_frequent_itemsets:\n",
        "                all_frequent_itemsets[itemset] = support_count_1[itemset] if k == 1 else 0\n",
        "        k += 1\n",
        "        candidates_k = generate_candidates(current_frequent_itemsets)\n",
        "        if not candidates_k:\n",
        "            break\n",
        "        support_count_k = get_support_count(candidates_k, basket_data)\n",
        "        current_frequent_itemsets = set()\n",
        "        for itemset, count in support_count_k.items():\n",
        "            if count >= min_support_count:\n",
        "                current_frequent_itemsets.add(itemset)\n",
        "                all_frequent_itemsets[itemset] = count\n",
        "\n",
        "    # Generate rules\n",
        "    rules = []\n",
        "    for itemset in all_frequent_itemsets:\n",
        "        if len(itemset) > 1:\n",
        "            for i in range(1, len(itemset)):\n",
        "                for antecedent in combinations(itemset, i):\n",
        "                    antecedent = frozenset(antecedent)\n",
        "                    consequent = itemset - antecedent\n",
        "                    if antecedent in all_frequent_itemsets:\n",
        "                        support_itemset = all_frequent_itemsets[itemset] / num_transactions\n",
        "                        support_antecedent = all_frequent_itemsets[antecedent] / num_transactions\n",
        "                        confidence = support_itemset / support_antecedent\n",
        "                        if confidence >= min_confidence:\n",
        "                            support_consequent = all_frequent_itemsets.get(consequent, 0) / num_transactions if consequent in all_frequent_itemsets else 0\n",
        "                            lift = confidence / support_consequent if support_consequent > 0 else 0\n",
        "                            rules.append({\n",
        "                                'antecedents': antecedent,\n",
        "                                'consequents': consequent,\n",
        "                                'support': support_itemset,\n",
        "                                'confidence': confidence,\n",
        "                                'lift': lift\n",
        "                            })\n",
        "\n",
        "    # Sort and filter rules\n",
        "    prominent_rules = [rule for rule in rules if rule['support'] >= min_support and rule['confidence'] >= 0.75]\n",
        "    prominent_rules = sorted(prominent_rules, key=lambda x: (x['confidence'], x['support']), reverse=True)\n",
        "\n",
        "    print(f\"Number of prominent rules: {len(prominent_rules)}\")\n",
        "    for rule in prominent_rules[:10]:  # Show top 10\n",
        "        ant = '-'.join(sorted(rule['antecedents']))\n",
        "        cons = '-'.join(sorted(rule['consequents']))\n",
        "        print(f\"Rule: {ant} -> {cons}, Support: {rule['support']:.2f}, Confidence: {rule['confidence']:.2f}, Lift: {rule['lift']:.2f}\")\n",
        "\n",
        "# To run:\n",
        "apriori_runner(min_support=0.05, min_confidence=0.6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcUS4YWB2-ww",
        "outputId": "1f1dc689-b82c-43ca-bf4c-fbfa428119da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of prominent rules: 34\n",
            "Rule: bread-milk-sugar -> butter, Support: 0.05, Confidence: 0.85, Lift: 1.56\n",
            "Rule: bread-coffee -> butter, Support: 0.10, Confidence: 0.84, Lift: 1.56\n",
            "Rule: butter-flour -> bread, Support: 0.10, Confidence: 0.84, Lift: 1.57\n",
            "Rule: bread-milk -> butter, Support: 0.11, Confidence: 0.84, Lift: 1.56\n",
            "Rule: butter-chocolate -> bread, Support: 0.10, Confidence: 0.84, Lift: 1.56\n",
            "Rule: bread-juice -> butter, Support: 0.12, Confidence: 0.84, Lift: 1.55\n",
            "Rule: bread-diapers -> butter, Support: 0.10, Confidence: 0.83, Lift: 1.54\n",
            "Rule: butter-milk-sugar -> bread, Support: 0.05, Confidence: 0.83, Lift: 1.55\n",
            "Rule: butter-juice -> bread, Support: 0.12, Confidence: 0.83, Lift: 1.54\n",
            "Rule: bread-rice -> butter, Support: 0.10, Confidence: 0.83, Lift: 1.53\n"
          ]
        }
      ]
    }
  ]
}